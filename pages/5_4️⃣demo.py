
import google.generativeai as genai 
import streamlit as st
from PIL import Image
import numpy as np
import requests
import xml.etree.ElementTree as ET
import json
from datetime import datetime
from ultralytics import YOLO  # YOLOv8 ë¡œë“œ
from ultralytics import settings
from dotenv import load_dotenv
import os
from typing import Optional, Tuple  # Optional ì¶”ê°€ 20241021
# from typing import Tuple 20241021
from PIL import Image # 20241021 
import re # 20241021 

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
my_api_key = os.getenv('GENAI_API_KEY')
ttbkey = os.getenv('ALADIN_API_KEY')

# Gemini API ë° ì•Œë¼ë”˜ API ì„¤ì •
genai.configure(api_key=my_api_key)

# Default book data in case API call fails or no results are found
DEFAULT_BOOK_DATA = {
    'title': 'ì†Œë…„ì´ ì˜¨ë‹¤',
    'author': 'í•œê°•',
    'pubDate': 'N/A',
    'description': 'N/A.',
    'isbn': '9788936434120',
    'isbn13': 'N/A',
    'priceSales': 'N/A',
    'priceStandard': 'N/A',
    'publisher': 'ì°½ë¹„',
    'cover': 'N/A',
    'salesPoint': 'N/A',
    'customerReviewRank': 'N/A'
}

# ì•Œë¼ë”˜ APIì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_book_data_by_isbn(isbn: str) -> Optional[dict]:
    url = f"http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?ttbkey={ttbkey}&itemIdType=ISBN&ItemId={isbn}&output=xml&Version=20131101&OptResult=usedList"
    
    try:
        response = requests.get(url)
        response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬

        root = ET.fromstring(response.content)
        namespace = {'ns': 'http://www.aladin.co.kr/ttb/apiguide.aspx'}
        items = root.findall('.//ns:item', namespace)

        if not items:
            st.write("ë„ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return DEFAULT_BOOK_DATA

        item = items[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
        book_data = {
            'title': item.find('ns:title', namespace).text or 'N/A',
            'author': item.find('ns:author', namespace).text or 'N/A',
            'pubDate': item.find('ns:pubDate', namespace).text or 'N/A',
            'description': item.find('ns:description', namespace).text or 'N/A',
            'isbn': item.find('ns:isbn', namespace).text or 'N/A',
            'isbn13': item.find('ns:isbn13', namespace).text or 'N/A',
            'priceSales': item.find('ns:priceSales', namespace).text or 'N/A',
            'priceStandard': item.find('ns:priceStandard', namespace).text or 'N/A',
            'publisher': item.find('ns:publisher', namespace).text or 'N/A',
            'cover': item.find('ns:cover', namespace).text or 'N/A',
            'salesPoint': item.find('ns:salesPoint', namespace).text or 'N/A',
            'customerReviewRank': item.find('ns:customerReviewRank', namespace).text or 'N/A'
        }
        return book_data
    except requests.RequestException as e:
        st.write(f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None

def clean_response_text(response) -> str:
    """
    ì…ë ¥ëœ í…ìŠ¤íŠ¸ì—ì„œ ë§ˆí¬ë‹¤ìš´ í¬ë§·(** ë˜ëŠ” __ ë“±) ì œê±°.
    ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜.
    """
    if not isinstance(response, str):
        # responseê°€ Noneì´ê±°ë‚˜ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return ""

    # ** ë˜ëŠ” __ë¡œ ê°ì‹¸ì§„ í…ìŠ¤íŠ¸ë¥¼ ì œê±°
    cleaned_text = re.sub(r"\*\*(.*?)\*\*", r"\1", response)  # **bold** ì²˜ë¦¬
    cleaned_text = re.sub(r"__(.*?)__", r"\1", cleaned_text)  # __italic__ ì²˜ë¦¬
    return cleaned_text

# Gemini APIë¥¼ ì‚¬ìš©í•´ ì±… ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_book_description(book_data: dict) -> str:
    prompt = (
        f"ì±… ì œëª©: {book_data['title']}\n"
        f"ì €ì: {book_data['author']}\n"
        # f"ì¶œíŒì¼: {book_data['pubDate']}\n"
        # f"ì±… ì„¤ëª…: {book_data['description']}\n"
        # f"íŒë§¤ ê°€ê²©: {book_data['priceSales']}ì›\n"
        f"ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ì±…ì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”\n" #ì±… ì œëª©ê³¼ ì €ìë¥¼ ì´ìš©í•´ ê²€ìƒ‰í•˜ì—¬ ì±…ì—ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
    )

    model = genai.GenerativeModel('gemini-1.5-flash')  # Gemini ëª¨ë¸ ì‚¬ìš© gemini-1.5-flash-002
    chat_session = model.start_chat(history=[])
    response = chat_session.send_message(prompt)

    # ì‘ë‹µì´ Noneì¼ ê²½ìš° ëŒ€ë¹„
    raw_response = response.text if response and hasattr(response, 'text') else ""

    # ë§ˆí¬ë‹¤ìš´ ì œê±° í›„ ë°˜í™˜
    cleaned_response = clean_response_text(raw_response)
    return cleaned_response

    # return response.text

# ë„ì„œ ì •ë³´ì™€ ì„¤ëª…ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
# def save_description_to_json(book_data: dict, book_description: str) -> None:
#     current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
#     file_name = f"book_description_{current_time}.json"

#     book_data["generated_description"] = book_description

#     with open(file_name, "w", encoding="utf-8") as file:
#         json.dump(book_data, file, ensure_ascii=False, indent=4)
#     print(f"ì„¤ëª…ê³¼ ì±… ì •ë³´ê°€ {file_name} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# YOLOv8 ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_yolo_model():
    return YOLO("small_best.pt")

yolo_model = load_yolo_model()

# front = Image.open("figures/front_cover.png")
# back = Image.open("figures/back_cover.png")
# spine = Image.open("figures/spine.png")
# page_edges = Image.open("figures/page_edges.png")
# ìƒ˜í”Œ ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •
SAMPLE_IMAGES = {
    "front": "figures/front_cover.png",
    "back": "figures/back_cover.png",
    "spine": "figures/spine.png",
    "page_edges": "figures/page_edges.png"
}


# if SAMPLE_IMAGES:
#     print("images are in dict")

#20241021 YOLO íƒì§€ ê²°ê³¼ ì²˜ë¦¬ í•¨ìˆ˜

def process_yolo_results(results) -> str:
    detected_classes = [r['name'] for r in results[0].boxes.data.cpu().numpy()]
    wear_count = detected_classes.count('wear')
    wet_count = detected_classes.count('wet')
    ripped_count = detected_classes.count('ripped')

    if wet_count > 0:
        return "ë§¤ì…ë¶ˆê°€"
    elif ripped_count > 0:
        return call_gpt_for_rip_length(results)  # ChatGPT í˜¸ì¶œí•˜ì—¬ ê¸¸ì´ ì¸¡ì •
    elif wear_count >= 2:
        return "ìƒ"
    elif wear_count == 1:
        return "ìµœìƒ"
    else:
        return "í’ˆì§ˆ ì •ë³´ ì—†ìŒ"

#20241021 # ChatGPT-4oë¥¼ í˜¸ì¶œí•˜ì—¬ ì°¢ì–´ì§„ ë¶€ìœ„ ê¸¸ì´ ì¸¡ì •
def call_gpt_for_rip_length(results) -> str:
    ripped_coordinates = [
        box['coordinates'] for box in results[0].boxes.data.cpu().numpy() if box['name'] == 'ripped'
    ]

#20241021 # GPT-4o ëª¨ë¸ì— ì°¢ì–´ì§„ ê¸¸ì´ ìš”ì²­
    prompt = (
        f"YOLO ëª¨ë¸ì´ íƒì§€í•œ ì°¢ì–´ì§„ ë¶€ìœ„ì˜ ì¢Œí‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
        f"{ripped_coordinates}\n"
        "ì´ ì¢Œí‘œë¥¼ ì´ìš©í•´ ì°¢ì–´ì§„ ê¸¸ì´ë¥¼ cm ë‹¨ìœ„ë¡œ ê³„ì‚°í•´ ì£¼ì„¸ìš”."
    )

    chat_session = genai.GenerativeModel('gpt-4o').start_chat(history=[])
    response = chat_session.send_message(prompt)

#20241021 # ê²°ê³¼ì—ì„œ ê¸¸ì´ ì¶”ì¶œ ë° ë°˜í™˜
    length = response.text.split()[0]  # ì˜ˆ: "3cm" -> "3" ì¶”ì¶œ
    return f"ì°¢ì–´ì§„ ê¸¸ì´: {length}cm"
        


# 20241021 ê°ì²´ íƒì§€ ë° ì‹œê°í™” í•¨ìˆ˜
def detect_and_annotate(image: Image.Image) -> Tuple[Image.Image, bool,Optional[float]]:
    image_np = np.array(image)  # PIL ì´ë¯¸ì§€ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
    results = yolo_model(image_np)  # íƒì§€ ìˆ˜í–‰

    detected = len(results[0].boxes) > 0  # íƒì§€ ì—¬ë¶€
    annotated_image = results[0].plot()  # íƒì§€ ê²°ê³¼ ì‹œê°í™”
# ì°¢ì–´ì§„ ë¶€ë¶„ì´ ìˆì„ ê²½ìš° ê¸¸ì´ ê³„ì‚°
    rip_length = None
    contains_wet = False  # wet ì¡´ì¬ ì—¬ë¶€ ì´ˆê¸°í™”


    for box in results[0].boxes.data.cpu().numpy():
        # Bounding boxì˜ ë„ˆë¹„ ë˜ëŠ” ë†’ì´ë¥¼ ê¸¸ì´ë¡œ ì‚¬ìš© (ì˜ˆì‹œ ê³„ì‚°)
        # 20241021 ì¶”ê°€
        class_index = int(box[5])  # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ëŠ” ë³´í†µ 5ë²ˆì§¸ ìœ„ì¹˜ì— ìˆìŒ
        class_name = yolo_model.names[class_index]  # í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜

        if class_name == 'ripped':
            # Bounding boxì˜ ì¢Œí‘œ ì¶”ì¶œ ë° ê¸¸ì´ ê³„ì‚°
            x1, y1, x2, y2 = box[:4]
            rip_length = round(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) / 10, 2)  # cm ë³€í™˜
        elif class_name == 'wet':
            contains_wet = True  # wetì´ ë°œê²¬ë˜ë©´ True ì„¤ì •

            # x1, y1, x2, y2 = box[:4]  # ì¢Œí‘œ ì¶”ì¶œ
            # rip_length = round(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) / 10, 2)  # cmë¡œ ë³€í™˜
    # í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ: íƒì§€ëœ ê°ì²´ì˜ í´ë˜ìŠ¤ ì´ë¦„ ëª¨ìŒ
    detected_texts = [yolo_model.names[int(box[5])] for box in results[0].boxes.data.cpu().numpy()]

    #return Image.fromarray(annotated_image[..., ::-1]), detected, rip_length  # RGB ë³€í™˜
    return Image.fromarray(annotated_image[..., ::-1]), detected, rip_length, detected_texts  # RGBë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜



# 20241021 ìˆ˜ì • YOLOv8 ê°ì²´ íƒì§€ í•¨ìˆ˜
def yoloinf(image: Image.Image) -> Tuple[str, Image.Image]:
    try: 
        image_np = np.array(image)
        results = yolo_model(image_np)
    # annotated_img = results[0].plot()  # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    # return Image.fromarray(annotated_img[..., ::-1])  # RGB í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        detected_classes = [yolo_model.names[int(cls)] for cls in results[0].boxes.cls]
        if "ripped" in detected_classes:
            quality = "ë§¤ì…ë¶ˆê°€"
        elif detected_classes.count("wear") >= 2:
            quality = "ìƒ"
        else:
            quality = "ìµœìƒ"

        annotated_img = results[0].plot()
        return quality, Image.fromarray(annotated_img[..., ::-1])
    except Exception as e:
        st.error(f"íƒì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return "ì˜¤ë¥˜", image

# í’ˆì§ˆ í‰ê°€ ë¡œì§ ì¶”ê°€
def generate_quality_evaluation(detection_results: list, rip_length: Optional[float] = None, detected_texts: dict = None) -> str:
    #detected_count = sum(int(result) for result in detection_results)
    detected_count = sum(1 for result in detection_results if result != "ìµœìƒ")
    #detected_count = sum(1 for result in detection_results if result != "ìµœìƒ")
    
    # # íƒì§€ëœ í…ìŠ¤íŠ¸ì—ì„œ 'ripped', 'wet', 'wear' ê°œìˆ˜ ì„¸ê¸°
    ripped_count = sum(
        texts.count("front_ripped") + texts.count("side_ripped")
        for texts in detected_texts.values()
    )
    wet_count = sum(
        texts.count("wet") for texts in detected_texts.values()
    )
    wear_count = sum(
        texts.count("wear") for texts in detected_texts.values()
    )

    # ì¡°ê±´ 1: wetì´ í•˜ë‚˜ë¼ë„ ê°ì§€ë˜ë©´ ë§¤ì…ë¶ˆê°€
    if wet_count >= 1:
        return "ë§¤ì…ë¶ˆê°€: ì –ì€ ì±…ì´ë¼ ë§¤ì…ë¶ˆê°€ì…ë‹ˆë‹¤."
    # ì¡°ê±´ 2: rippedê°€ 2ê°œ ì´ìƒ ê°ì§€ë˜ë©´ ë§¤ì…ë¶ˆê°€    
    if ripped_count >= 3:
        return "ë§¤ì…ë¶ˆê°€: íƒì§€ëœ ì°¢ì–´ì§„ ë¶€ìœ„ê°€ 2cm ì´ìƒì…ë‹ˆë‹¤."
     # ì¡°ê±´ 3: wearë§Œ ê°ì§€ëœ ê²½ìš°
    if wear_count > 0 and ripped_count == 0 and wet_count == 0:
        if wear_count == 1:
            return "ìµœìƒ: wear ì†ìƒì´ 1ê°œë§Œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        elif wear_count == 2:
            return "ìƒ: wear ì†ìƒì´ 2ê°œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."

    prompt = (
        f"íƒì§€ëœ ì´ë¯¸ì§€ ìˆ˜: {detected_count}/4\n"
        "ì±…ì˜ í’ˆì§ˆ ë“±ê¸‰ì„ í‰ê°€í•´ ì£¼ì„¸ìš”:\n"
        "- ìµœìƒ: ìƒˆê²ƒì— ê°€ê¹Œìš´ ì±…\n"
        "- ìƒ: ì•½ê°„ì˜ ì‚¬ìš©ê°ì´ ìˆìœ¼ë‚˜ ê¹¨ë—í•œ ìƒíƒœ\n"
        "- ì¤‘: ë³€ìƒ‰ ë° ì•½ê°„ì˜ ì†ìƒ ìˆìŒ\n"
        "- ë§¤ì…ë¶ˆê°€: ì‹¬í•œ ì†ìƒ ë˜ëŠ” ì˜¤ì—¼\n"
    )
# 20241021 ì‹œì‘
    if rip_length:
        prompt += f"\nì°¢ì–´ì§„ ê¸¸ì´: {rip_length}cm\n"
    
    # íƒì§€ëœ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í¬í•¨
    if detected_texts:
        prompt += "\níƒì§€ëœ í…ìŠ¤íŠ¸ ì •ë³´:\n"
        for key, texts in detected_texts.items():
            prompt += f"{key}: {', '.join(texts)}\n"
 
    prompt += "\n ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì±…ì˜ í’ˆì§ˆì„ í‰ê°€í•´ ì£¼ì„¸ìš”. í˜¹ì‹œ ì†ìƒëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì–´ë–¤ ë¶€ë¶„ì´ ì†ìƒë˜ì—ˆëŠ”ì§€ ìì„¸í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”."
# 20241021 ë—

    model = genai.GenerativeModel('gemini-pro')
    chat_session = model.start_chat(history=[])
    response = chat_session.send_message(prompt)

    return response.text

# JSONìœ¼ë¡œ ì €ì¥
# def save_description_to_json(book_data: dict, book_description: str) -> None:
#     current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
#     file_name = f"book_description_{current_time}.json"
#     book_data["generated_description"] = book_description

#     with open(file_name, "w", encoding="utf-8") as file:
#         json.dump(book_data, file, ensure_ascii=False, indent=4)
#     st.write(f"{file_name}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# main() í•¨ìˆ˜ë¡œ ì½”ë“œë¥¼ êµ¬ì¡°í™”

# Streamlit ì•±
def main():
    st.title("Aladin-Bot")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ“š ì±… ì •ë³´ ì¡°íšŒ"):
            st.session_state["show_isbn_input"] = True

    with col2:
        if st.button("â­ íŒë§¤ ë“±ê¸‰ íŒì •"):
            st.session_state["show_upload"] = True

    if st.session_state.get("show_isbn_input", False):
        isbn = st.text_input("ISBNì„ ì…ë ¥í•˜ì„¸ìš”")
        if isbn:
            book_data = get_book_data_by_isbn(isbn)
            if book_data:
                book_description = generate_book_description(book_data)
                st.write("ìƒì„±ëœ ì„¤ëª…:", book_description)
                #save_description_to_json(book_data, book_description)

    if st.session_state.get("show_upload"):
        st.write("ì•„ë˜ ìƒ˜í”Œ ì‚¬ì§„ì„ ì°¸ê³ í•´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”:")

        # ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ 2ê°œì”© í•œ ì¤„ì— í‘œì‹œ
        cols = st.columns(2)
        with cols[0]:
            st.image(SAMPLE_IMAGES["front"], caption="ì•í‘œì§€ ì˜ˆì‹œ", width=200)
            st.image(SAMPLE_IMAGES["spine"], caption="ì±…ë“± ì˜ˆì‹œ", width=200)
        with cols[1]:
            st.image(SAMPLE_IMAGES["back"], caption="ë’·í‘œì§€ ì˜ˆì‹œ", width=200)
            st.image(SAMPLE_IMAGES["page_edges"], caption="ì±…ë°° ì˜ˆì‹œ", width=200)

        # ì‚¬ìš©ì ì´ë¯¸ì§€ ì—…ë¡œë“œ
        front_cover = st.file_uploader("ì•í‘œì§€", type=["jpg", "png", "jpeg"])
        back_cover = st.file_uploader("ë’·í‘œì§€", type=["jpg", "png", "jpeg"])
        spine = st.file_uploader("ì±…ë“±", type=["jpg", "png", "jpeg"])
        page_edges = st.file_uploader("ì±…ë°°", type=["jpg", "png", "jpeg"])

        if front_cover and back_cover and spine and page_edges:
            st.success("ì´ë¯¸ì§€ê°€ ëª¨ë‘ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


            # ê° ì´ë¯¸ì§€ íƒì§€ ë° ì‹œê°í™” ìˆ˜í–‰ 20241021

            
            detected_front, front_found, front_rip, front_texts = detect_and_annotate(Image.open(front_cover))
            detected_back, back_found, back_rip, back_texts = detect_and_annotate(Image.open(back_cover))
            detected_spine, spine_found, spine_rip, spine_texts = detect_and_annotate(Image.open(spine))
            detected_page_edges, edges_found, edges_rip, edges_texts = detect_and_annotate(Image.open(page_edges))

            #detected_front, front_found, front_rip = detect_and_annotate(Image.open(front_cover))
            # detected_front, front_found, front_texts = detect_and_annotate(Image.open(front_cover))
            # st.image(detected_front, caption="ì•í‘œì§€ íƒì§€ ê²°ê³¼")
            # detected_back, back_found, back_rip = detect_and_annotate(Image.open(back_cover))
            # detected_spine, spine_found, spine_rip = detect_and_annotate(Image.open(spine))
            # detected_page_edges, edges_found, edges_rip = detect_and_annotate(Image.open(page_edges))

            # íƒì§€ëœ ì´ë¯¸ì§€ í‘œì‹œ
            st.image(detected_front, caption="ì•í‘œì§€ íƒì§€ ê²°ê³¼")
            st.image(detected_back, caption="ë’·í‘œì§€ íƒì§€ ê²°ê³¼")
            st.image(detected_spine, caption="ì±…ë“± íƒì§€ ê²°ê³¼")
            st.image(detected_page_edges, caption="ì±…ë°° íƒì§€ ê²°ê³¼")
                # íƒì§€ëœ í…ìŠ¤íŠ¸ ì •ë³´ ì¶œë ¥
            # if front_texts:
            #     st.write("ì•í‘œì§€ì—ì„œ íƒì§€ëœ ê°ì²´ë“¤:")
            #     st.write(", ".join(front_texts))  # íƒì§€ëœ ê°ì²´ ì´ë¦„ë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥
            # else:
            #     st.write("ì•í‘œì§€ì—ì„œ íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            # st.write("ì•í‘œì§€ íƒì§€ ê²°ê³¼:", ", ".join(front_texts))
            # st.write("ë’·í‘œì§€ íƒì§€ ê²°ê³¼:", ", ".join(back_texts))
            # st.write("ì±…ë“± íƒì§€ ê²°ê³¼:", ", ".join(spine_texts))
            # st.write("ì±…ë°° íƒì§€ ê²°ê³¼:", ", ".join(edges_texts))
            # 2024102 ì£¼ì„ ì²˜ë¦¬ íƒì§€ ê²°ê³¼ ì €ì¥ (boolean ê°’ë§Œ í¬í•¨)
           # detection_results = [front_found, back_found, spine_found, edges_found]
            # 20241021 # ì°¢ì–´ì§„ ê¸¸ì´ ì¤‘ ê°€ì¥ ê¸´ ê°’ ì„ íƒ
            rip_length = max(
                filter(None, [front_rip, back_rip, spine_rip, edges_rip]), default=None
            )
            # íƒì§€ëœ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬
            detected_texts = {
                "ì•í‘œì§€": front_texts,
                "ë’·í‘œì§€": back_texts,
                "ì±…ë“±": spine_texts,
                "ì±…ë°°": edges_texts,
            }

            
            # í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰
            detection_results = [front_found, back_found,spine_found, edges_found] 
            st.write("Gemini í’ˆì§ˆ í‰ê°€ ìš”ì²­ ì¤‘...")
            quality_result = generate_quality_evaluation(detection_results, rip_length, detected_texts)
            st.write("í’ˆì§ˆ í‰ê°€ ê²°ê³¼:", quality_result)
        else:
            st.warning("4ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")



# main í•¨ìˆ˜ ì‹¤í–‰
if __name__ == "__main__":
    main()

